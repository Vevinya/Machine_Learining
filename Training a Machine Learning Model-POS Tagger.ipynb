{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Program for feature extraction\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(string):              #This function is used to remove punctuation from string\n",
    "    clearstring=\"\"\n",
    "    if(string.isalnum()==False):             #This statement returns False if the string has punctuation\n",
    "        for j in range (len(string)):        #Now we are checking if every letter in the string is a punctuation or not\n",
    "            if (string[j].isalnum()==False):\n",
    "                if(j==len(string)-1):\n",
    "                    if string[j]==\"!\":\n",
    "                        clearstring=str(clearstring)+string[j]\n",
    "                continue\n",
    "            else:\n",
    "                clearstring=str(clearstring)+string[j]  #It concatenate only the alphanumeric words\n",
    "        return clearstring\n",
    "    else:\n",
    "        return string\n",
    "\n",
    "from nltk.corpus import brown\n",
    "wordlist=list(brown.words(categories='editorial'))   #storing all words from dataset \n",
    "words=[remove_punctuation(w) for w in wordlist]      #removing punctuation uinf function\n",
    "words_unique=set(words)                              #extracting only the unique words\n",
    "ref_words=[w for w in words_unique if len(w)>1]      #taking words whose length is greater than 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(word):\n",
    "    return{\n",
    "    'last_letter':word[-1],     \n",
    "    'noun': (word[0].upper()==word[0] and word[1:]==word[1:].lower()) or word[-4:]in[\"ment\",\"ness\",\"ence\",\"ance\",\"hood\",\"ship\"] or word[-3:]==\"ity\",\n",
    "    'plural': word[len(word)-1]=='s',\n",
    "    'interjunction':word[-1]=='!',\n",
    "    'adverb':word[-2:]==\"ly\",\n",
    "    'number':str(word).isdigit(),\n",
    "    'wh_word':word[:2]=='wh',\n",
    "    'article': word in [\"an\",\"the\",\"a\"], #Checks if the word is an article\n",
    "    'gerund':word[-3:]==\"ing\",\n",
    "    'verb_past':word[-2:]==\"ed\",\n",
    "    'proper_adjective':word[-2:]==\"an\",\n",
    "    'be_verb':word in [\"am\",\"is\",\"are\",\"was\",\"were\"],\n",
    "    'reflexive_pronoun':word[-4:]==\"self\" or word[-6:]==\"selves\",\n",
    "    'possessive pronoun':word[-2:]==\"rs\",      #Checks if last two letters end in \"rs\"\n",
    "    'subject_pronoun':word in [\"I\",\"you\",\"he\",\"she\",\"it\",\"we\",\"you\",\"they\"],\n",
    "    'coordinating_conjunction':word in [\"and\",\"but\",\"for\",\"or\",\"not\",\"yet\",\"so\"],\n",
    "    'all_lower':str(word).lower()==word,  #Checks if all letters in word are in lowercase\n",
    "    'all_upper':str(word).upper()==word,  ##Checks if all letters in word are in uppercase\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_words=brown.tagged_words()\n",
    "tag_words1=list(set(tag_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def untag(tag_word):\n",
    "    return [w for w,x in tag_words1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_dataset(list_words):\n",
    "    X,y=[],[]\n",
    "    for word in list_words:\n",
    "        X.append(features(untag(word)))\n",
    "        y.append(word[1])\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut=int(0.75*len(tag_words1))\n",
    "training_words = tag_words1[:cut]\n",
    "testing_words = tag_words1[cut:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50204\n",
      "16735\n"
     ]
    }
   ],
   "source": [
    "print(len(training_words))\n",
    "print(len(testing_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "X,y=transform_to_dataset(training_words[:500])\n",
    "print(len(X))\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vectorizer',\n",
       "                 DictVectorizer(dtype=<class 'numpy.float64'>, separator='=',\n",
       "                                sort=True, sparse=False)),\n",
       "                ('classifier',\n",
       "                 DecisionTreeClassifier(class_weight=None, criterion='entropy',\n",
       "                                        max_depth=None, max_features=None,\n",
       "                                        max_leaf_nodes=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        presort=False, random_state=None,\n",
       "                                        splitter='best'))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier=Pipeline([\n",
    "    ('vectorizer',DictVectorizer(sparse=False)),\n",
    "    ('classifier',DecisionTreeClassifier(criterion='entropy'))\n",
    "])\n",
    "classifier.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
