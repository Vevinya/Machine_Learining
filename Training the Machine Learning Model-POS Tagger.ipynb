{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating and training a machine learning model\n",
    "import nltk\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function for feature extraction\n",
    "def features(word):\n",
    "    return{\n",
    "    'last_letter':word[-1],\n",
    "    'first_letter_upper': str(word[0]).upper()==word[0],\n",
    "    'rem_letter_lower': word[1:]==str(word[1:]).lower(),\n",
    "    'number': str(word).isdigit(),\n",
    "    'all_lower':str(word).lower()==word,\n",
    "    'all_upper':str(word).upper()==word,\n",
    "    'prefix-1': word[0],\n",
    "    'prefix-2': word[:2],\n",
    "    'prefix-3': word[:3],\n",
    "    'suffix-1': word[-1],\n",
    "    'suffix-2': word[-2:],\n",
    "    'suffix-3': word[-3:],\n",
    "    'suffix-4': word[-4:],\n",
    "    'suffix-6': word[-6:],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_words=brown.tagged_words()\n",
    "tag_words1=list(set(tag_words))\n",
    "tag_word=[w for w in tag_words1 if len(w)>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_dataset(list_words):\n",
    "    X,y=[],[]\n",
    "    for word in list_words:\n",
    "        for j in range(len(word)//2):\n",
    "            X.append(features(word[0]))\n",
    "            y.append(word[1])\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#60% of data for training and 20% for validating and testing dataset\n",
    "cut1=int(0.6*len(tag_word))\n",
    "cut2=int(0.8*len(tag_word))\n",
    "training_words = tag_word[:cut1]\n",
    "validation_words = tag_word[cut1:cut2]\n",
    "testing_words = tag_word[cut2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40163\n",
      "13388\n",
      "13388\n"
     ]
    }
   ],
   "source": [
    "print(len(training_words))\n",
    "print(len(validation_words))\n",
    "print(len(testing_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=transform_to_dataset(training_words[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "print(len(X))\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vectorizer',\n",
       "                 DictVectorizer(dtype=<class 'numpy.float64'>, separator='=',\n",
       "                                sort=True, sparse=False)),\n",
       "                ('classifier',\n",
       "                 DecisionTreeClassifier(class_weight=None, criterion='entropy',\n",
       "                                        max_depth=None, max_features=None,\n",
       "                                        max_leaf_nodes=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        presort=False, random_state=None,\n",
       "                                        splitter='best'))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier=Pipeline([\n",
    "    ('vectorizer',DictVectorizer(sparse=False)),\n",
    "    ('classifier',DecisionTreeClassifier(criterion='entropy'))\n",
    "])\n",
    "classifier.fit(X,y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
